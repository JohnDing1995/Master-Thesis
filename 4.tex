\chapter{Design of DevOps Toolchains}
In this chapter, we introduce the design and implementation of both toolchains (integrated and non-integrated) and explain how we come to this implementation. We will also compare these two types of toolchains within the scope of functionality and ease of implementation.
Note that for the experiments that are answering our two research questions in the next chapter, we implement two different continuous delivery pipelines design with two sets of tools respectively, one with tradition non-integrated tool while another one with the serverless integrated DevOps tools from AWS.
\par
In Section 4.1, we present a case software project that uses DevOps toolchains in the experiments. In Section 4.2, we introduce the design and implementation of our non-integrated DevOps toolchain. Section 4.3 is related to the integrated toolchain, and Section 4.4 is a comparison between integrated and non-integrated toolchain. Lastly, in Section 4.5, we talk about the challenges we met during the implementation.
\section{Case Project}
We first develop the case project. The case project is an example software project which will be used to test our implementation and run the experiments in which we simulate the DevOps development process of the case project on our DevOps toolchain. 
The case project is a REST API service because such services are an essential part of modern software projects with microservices architecture. 
Although the type of case project has no impact on our DevOps toolchain at the architectural level, the build dependencies and software configuration in the toolchain may be affected. Thus we need to have an introduction to the case project.
\par
We choose Java as the programming language used for the case project because it's popularity and versatility.
Java is one of the most common languages used in commercial software development. According to the TIOBE index of programming language \cite{indexTIO42:online}, Java is the most popular or second most popular programming language in the world since the mid-1990s. Besides commercial software development, the Java programming language is also widely used in open-source software development. The report \cite{TheState3:online} from GitHub shows that Java ranked third in 2019 and second before 2018. One of the main applications of Java in Web development. Currently, 7 out of every 10 \cite{Programm17:online} most popular websites in the world use Java as the web development language (server-side). Furthermore, Java has good versatility, which means that it is suitable for almost all types of applications. For example, web applications, desktop applications, and besides, Java is the main development language for Android applications.
\par
Java programming language has a whole ecosystem that can be used to improve software development practices for adopting DevOps. These tools include: build, code analysis, testing frameworks, artifact management, build automation \& dependency management et. These tools could be easily integrated and act as part of the DevOps toolchain.
\par
In term of developing REST API with Java, the Spring is the most popular framework and has been used in many major Internet companies, including Google, Microsoft and Amazon \cite{SpringWh14:online}.
So, we choose Spring as the framework to build our application. To develop our Spring application, we use Spring Boot\footnote{https://spring.io/projects/spring-boot}. Spring Boot is a project under Spring, which, according to its documentation, is to allow the developer to create Spring application with the minimal effort \cite{SpringBo84:online}, by simplifying the configuration of Spring framework.
\begin{figure}[!h]
     \begin{verbatim}
     Method: GET
     Endpoint: /packages
     Success Response:
     Code: 200
     Content:
     [
          {
               name : (Package name)
               description : (Package description)
               dependencies : (Dependencies)
          }
     ]
     Error Response:
     Code: 500
     Content: { msg: Server Error! }
     \end{verbatim}
     \caption{RESTful API Interface of Case Project}
     \label{fig:rest}
     \end{figure}
\par
The case project is a simple REST API (Figure \ref{fig:rest}) which returns the info of all installed software packages in the host machine in JSON format when the frontend sends an HTTP GET request to the backend.
\section{Design of Non-integrated DevOps Toolchain}
In this section, we introduced the design of a non-integrated DevOps toolchain. We first introduce the considerations when choosing the tools to build the toolchain. When introducing each component of the toolchain, we will also introduce how the component uses serverless computing.
\subsection{Architecture}
The toolchain implementation is based on the DevOps elements we presented in Chapter 2. Figure \ref{fig:archjenkins} shows the architecture of our DevOps toolchain. Here we are only presenting architecture on a more general level. The detailed architecture of each component will be introduced in the following sections, both text and graph. From Figure \ref{fig:archjenkins}, we can see except version control, and the whole environment is running in Amazon Web Services. Due to the limitation of space, the internal architecture of certain components is not shown in the graph. Instead, we will show them in the following sections.
\begin{figure}[!h]
     \centering
     \includegraphics[width=0.99\textwidth]{pics/arch-med-jenkins.png}
     \caption{Architecture diagram of the non-integrated DevOps toolchain}
     \label{fig:archjenkins}
\end{figure}
\par
When the developer pushes a new commit to the repository in GitHub \footnote{https://github.com/}, Github sends a HTTP POST request that contains the necessary information to the Jenkins master node. Jenkins master, which triggered by the HTTP request, will create a new job for this project according to the information that the HTTP request contains. The job will first pull the latest code from the git repository, then runs the docker containers with required build environment and build the project. In the end, a docker image for running the project will be created and be pushed to the container registry of AWS. The project will be deployed in the last step.
\subsection{Introduction to Tools Used in the Implementation}
One of the essential steps to build the non-integrated toolchain is to select the proper tool for each component.
In this section, we describe our consideration when we select tools.
\subsubsection{Continuous Delivery Pipeline}
The most popular server-based tools for build continuous delivery pipeline are Jenkins\footnote{https://www.jenkins.io/}, Drone\footnote{https://drone.io/}, GoCD\footnote{https://www.gocd.org/} and Circle CI\footnote{https://circleci.com/}. Table \ref{tab:ci-tools} shows a comparison between these tools. As we can see from the table, Jenkins is the most popular option for CI/CD. Jenkins has wide application in the commercial use case, and the high popularity in the open-source community as well. Although compared with the other three newer tools, Jenkins is more focuses on the "Build" step within the continuous delivery pipeline since it was originally a build automation server. But, the open-source nature of Jenkins gives it a much wider selection of the plugin, which means Jenkins can be used for almost all steps in a continuous delivery pipeline.
\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|}
\hline
& Jenkins & Drone & Circle CI & GoCD \\ \hline
Open Source & Yes & Yes & No & Yes \\ \hline
GitHub stars & 15.7k & 21.2k & - & 5.7k \\ \hline
Github contributors & 614 & 258 & - & 116 \\ \hline
Plugin extensions &
Over 1500 \tablefootnote{https://plugins.jenkins.io/} &
93 \tablefootnote{According to GitHub search result} &
110 \tablefootnote{https://circleci.com/integrations/} &
88 \tablefootnote{https://www.gocd.org/plugins/} \\ \hline
\begin{tabular}[c]{@{}l@{}}Price of self-hosted \\ solution\end{tabular} &
Free &
Free &
\$35 user/month &
Free \\ \hline
\begin{tabular}[c]{@{}l@{}}Number of companies\\use it in the tech stack\tablefootnote{based on data from StackShare}\end{tabular} &
2634 &
82 &
1368 &
42 \\ \hline
\end{tabular}
\caption{Comparison of continuous delivery tools}
\label{tab:ci-tools}
\end{table}
\par
Created by Kohsuke Kawaguchi in 2001 as "Hudson", Jenkins is an open-source automation server write with Java, which helps to automate different parts in the software development life cycle.
It is suitable for a team of all sizes and varies of languages and technologies \cite{smart2011jenkins}. Furthermore, Jenkins attracts software teams with its ease of use and high extensibility \cite{smart2011jenkins} and thousands of plugins. Since Jenkins has an active open source community, more plugins are being created and maintained. These plugins can help Jenkins keep up with the rapidly evolving DevOps practices and help Jenkins integrate with emerging tools and cloud services. extensibility makes Jenkins still the most popular tool in the DevOps toolchain, even if it is an aged software created when the term "DevOps" appeared.
\par
Our continuous delivery pipeline is developed with Pipeline plugin\footnote{https://www.jenkins.io/doc/book/pipeline/} in Jenkins.
Pipeline plugin allows us to define a continuous delivery pipeline as code in Jenkinsfile.
In the pipeline, a conceptually distinct subset of tasks within the continuous delivery pipeline \cite{Pipeline85:online} is defined as a "stage"\footnote{For example, "Build", Test", "Deploy" step in a continuous delivery pipeline.} and each task within a step is called "step". Each pipeline is binding with a "project". An execution runtime of a project/pipeline is called "build", and the machine (virtual machine, container, etc.) for running the build is called "agent".
\subsubsection{Build \& Test Automation Tool}
Within this project, we use Gradle\footnote{https://gradle.org/} as the build tool.
Gradle is a powerful build tool that was originally designed for JVM-based languages, but now it also supports other programming languages such as C++ and Python.
Like Jenkins, Gradle also has a dynamic ecosystem with a large number of plugins. This makes it possible to use different types of tools (such as unit testing and code analysis) in a single pipeline in Gradle. 
Moreover, Gradle makes dependency management easy, and dependencies can be easily added to the project by editing the project's Gradle configuration file. In addition, Gradle supports the configuration as code. This allows developers to define all build configurations of a software project in one file.
\par
For unit testing within the build stage, we are using JUnit \footnote{https://junit.org/} as the tool for testing. JUnit is a test framework which allows us to specify unit test by within @Test annotation within the Java Code. The JUnit executes the unit test amount three approaches of test automation we mentioned at \ref{TestA}. The "unit" means the smallest component of software, usually a method in practice \cite{UnitTest65:online}. A unit test is a testing approach which checks if each part of the software works properly. 
This is an early test conducted in the DevOps life cycle, and it will be conducted during the build. Therefore, we embed JUnit into the Gradle build process. Using JUnit for unit testing can improve the confidence of developers when developing software \cite{UnitTest65:online}. Frequent and early unit testing enables developers to find errors earlier. The earlier errors are found, the easier it is to fix them. Furthermore, the unit test helps the software team to be confident that the new change that passes the test will not break the existing product. This could make the team be more dare to make the change, do faster releases and be agile.
\par
For code analysis, we use SonarScanner for Gradle\footnote{https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-gradle/}. SonarScanner provided an easy way to embed SonarQube code analysis into the Gradle build process. This means in our implementation, and we use the tool to do static analysis on our Java code to build our project with Gradle. Static analysis means the code analysis tools will analysis our source code but without build and execute the code. The analysis objects include software structure, security, code quality, etc. After the analysis is done, a report generated by SonarQube could give a software development team complete overview of the code quality issues. 
SonarQube also quantifies the code quality by gives test case coverage ratio and technical debt ratio to the code. The quantitative code quality metrics make it easy for the team to enforce the quality policy. A common way to enforce the quality policy with metrics is to have a quality gate in the pipeline, which decides the project is "good enough to deliver" when metrics reach a certain threshold.
The code analysis, on the one hand, automates the code review process, and enforce code quality policy, on the other hand, helps to maintain code quality in the very early of the DevOps pipeline. Thus, code analysis reflects the philosophy of DevOps: automation, fast software delivery with high quality.
\subsubsection{Deployment and Jenkins Agents}
We will widely use Docker\footnote{https://www.docker.com/} in our pipeline. Docker is an open-source software which could pack, deliver and run the software as a container with OS-level visualization. Docker is developed by Docker, Inc. and published at PyCone in California, the USA in 2013. In March 2013, Docker became an open-source software \cite{avram2013docker}. Docker has high compatibility that allows the user to run Docker in any major operating systems(macOS, Linux and Windows) with both X86 and ARM CPU architectures.
\par
To software teams, Docker eases the environment configuration task by replacing VM with lightweight containers.
A container is a separate unit that includes the application and all its dependencies which allow application runs in the same way regardless of the host environment \cite{WhatisaC60:online}. A container is the running instance of a Docker image that defined by Dockerfile. Compared with VM, which emulate the hardware and wraps up the whole operation system, containers only includes the dependency needed to run an application. Multiple containers could run on tops of one Linux instance. This means the container is lightweight and cost much fewer resources. With the same hardware, the team could run 4x-6x numbers of application in the container than with VM \cite{vaughan2014docker}.

Docker fits well with DevOps since it allows the software team to quickly create local development environments that simulate different production environments in a single machine. For example, by creating multiple containers with different operating systems on the build machine, the development team can build C/C++ applications for different OS (Windows, macOS) only on the build machine with Linux system.
In addition, Docker allows the team to deploy to the cloud more easily \cite{vaughan2014docker}. Furthermore, Docker is designed so it could easily corporate with many DevOps tools \cite{Whosusin96:online}. In short, Docker can easily meet the needs of DevOps: it can easily run multiple development environments on the same host, is easy to deploy to the cloud, is very recourse-saving compared with VM, and fits well with DevOps tools.
\par
Docker allows us to specify all system dependencies in a single file (Dockerfile). Dockerfile defined the built environment as code. This allows us to manage the build environment with the version control system, and to test and do quality analysis to the environment defined by Dockerfile. In short, the Dockerfile could improve the automation within the system and help to apply DevOps practices on the build environments.
\par
\label{docker}
There will be two main use cases of Docker in our toolchain. Firstly, we run the build stage within the container.
This means the pipeline will execute specific steps inside ephemeral Docker containers \cite{Overview44:online}. It is easier to manage build dependencies in the Docker container. Besides, the container-based agent requires less effort to maintain.
\par
In our case, to build the case application, the host machine needs to have JVM installed. However, we want to make our pipeline not only suitable for Java application but also easily be used to build an application in other programming languages. Docker solves this problem by provides excellent isolation from the host machine. Thus, we can configure the built environment (operating system version, dependencies) runs within a Docker container without actually install anything on the host machine by merely editing the Dockerfile.
\par
Second, we use Docker to Dockerize our application which creates a Docker image of our application. The first reason for using Docker is that Docker reduces the operational effort.
With Docker, there is no need to pre-install any Java environment in the environment to run our application. This is because all environment is already being packed in our Docker image. 
The second reason, Docker improved compatibility of the case project because Docker ensures that the application packed container can run in the same behaviour no matter what host it is running on.
The last reason, all major cloud computing providers support Docker. 
We can easily run the container on the cloud services. This means that our Dockerized applications can be easily cloud-native and can be deployed in a multi-cloud environment. For example, in AWS, there are Elastic Container Services (introduced in Chapter 3) specifically for container orchestration.
\subsubsection{Monitoring}
We are using CloudWatch for monitoring the status of the cloud infrastructure. CloudWatch is a tool provided by AWS, which we already introduced in Chapter 3. However, CloudWatch cannot give us insight about the status of Java SpringBoot application. More generally, the metrics within the application framework. In Spring Boot, these metrics are HTTP statistics, CPU load and JVM statistics. Prometheus \footnote{https://prometheus.io/}, together with Grafana, could fill this gap.
\par
Prometheus is an open-source monitoring and alerting solution initially built by SoundCloud in 2012 \cite{Overview30:online}. Prometheus is used for reading numeric metrics that are recorded in time series. In Spring Boot, there are some plugins, for example, Micrometer \footnote{https://micrometer.io/} exist which could export all the Spring Boot specific metrics to Prometheus in time series. Thus Prometheus has a perfect fit with our case project. In addition to collecting metrics, we could also set the alarm within Prometheus, which could alert the user when some metrics are not in the normal range. we can also query the metrics from the past. Although Prometheus supports simple graph which shows the metrics' change with time, it is not user friendly enough. Thus, we introduce Grafana to better visualizes the data collected by Prometheus.
\par
Grafana is an observation platform in which it's core feature is in the visualization. Users could define dashboards according to their needs with JSON files. Another main feature if Grafana is that it could gather data from the different platform into one dashboard. In our case, with Grafana, we can display the metrics from Prometheus and CloudWatch in a single page. This could give us a great overview of both Spring Boot application and cloud infrastructure at the same time.
\subsection{Infrastructure as Code (IaC)}
Infrastructure as code the common practices to implement configuration Management in the cloud-based environment. And Configuration management is one of the components of the DevOps toolchain that we mentioned in Chapter 2.
\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{pics/terraform.png}
\caption{Creating a cloud environment with Terraform CLI}
\label{fig:terraform}
\end{figure}
\par
Terraform \footnote{https://www.terraform.io/} is one of the most popular tools to manage cloud Infrastructure with infrastructure as code practice. Infrastructure as code enables the software team to version control and test the infrastructure to track its change. Furthermore, it enables the team to apply DevOps practice, such as test automation on the infrastructure.

Terraform is not limited to defined infrastructure as code; it also helps to do the cloud infrastructure orchestration. Compared with tools like Ansible, which the code defines each step of the automated process, Terraform code only define the final status of the infrastructure. This makes the code more understandable. Also, Terraform is portable, means it support infrastructure orchestration of all major cloud providers

In our implementation, we define our cloud infrastructure and all AWS resources, including EC2 virtual machine, ECS cluster, security groups and network Infrastructures in a series of configuration files. Then we create the cloud environment by simply using CLI interfaces. Figure \ref{fig:terraform} shows the creation of the cloud environment with Terraform.
\subsection{Version Control}
\label{vcs}
Version Control System (VCS) is the process that record the changes in source code set over time \cite{GitAbout93:online}, and versioning the history of these files. 
Version control can not only apply to the software source code, but also the cloud infrastructure, build configuration, Docker images, continuous integrating pipelines that we define as code in our project.
VSC is suitable for track the development progress and manages the goal within a software development team \cite{loeliger2012version}. Among all software for version control, Git is the most popular one nowadays.
Git is a distributed version control software created by Linus Torvalds. Git is based on the command-line interface (CLI), which allow the user to execute commands in different environments, for example, within continuous integration pipeline, within Gradle build or within a Docker container.

The survey \cite{CompareR31:online} from Synopsys shows that in 2019, 71\% of the project today is using Git as it is versioning system while SVN that ranks in second only be used in 25\% of the projects. We use Git as the version control system since it is used by most of the software development teams nowadays. We use GitHub for hosting the case project. Github is the biggest platform in the world that hosting a version-controlled software project for free using Git. It provides interfaces with different DevOps related tools which makes it easy to be integrated into all kinds of DevOps toolchains. The setup of Git includes the client, which is the development machine, and the remote server, which is the server that hosting the file history on the network. A branch means a diverge from main codebase which allows the developer to work on it without touch the code in the main codebase \cite{GitBranc33:online}. When the developer adds now code to on the feature branch, the code in the main branch (main codebase) might also be changed by other developers. And when the developer what to submit the change to the main branch, he/she has to uses merge. Merge is to combine two branches, by combining two sets of commits in two branches to a unified history \cite{GitMerge0:online}. 
\begin{figure}[h]
\centering
\includegraphics[width=0.99\textwidth]{pics/git.png}
\caption{GitHub Workflow \cite{guides2013understanding}}
\label{fig:git}
\end{figure}
\par
The Git flow \cite{driessen2010successful} proposed in 2010 is a successful workflow for working with Git. Git flow has already been widely used and has been approved by the software industries. However, the git-flow is centred with "release" which makes it not suitable for a DevOps team which could deploy to production everyday\cite{guides2013understanding}. To better cope with the frequent release nature of DevOps, the Github flow -- a simplified version of Git flow is proposed by GitHub. Github flow is light and branch-based workflow that the team could follow for collaborating on a software project. Compared with git-flow, GitHub flow 
Therefore, we choose GitHub flow \cite{chacongithub} the basis of our workflow in the project. The simplified version of this GitHub flow is shown as in Figure \ref{fig:git} 
\par
Several general principles followed by us when adapting GitHub flow, we refer to principals in \cite{chacongithub} to design our workflow.
\begin{itemize}
\item When working on the new feature, make a new branch for this feature. The name of this branch should be descriptive, which reflect the content of this feature. Commit the new code related to this feature to the feature branch. And push from this feature branch to the branch with the same name on the remote server (github.com) when necessary.
\item Open a pull request\footnote{https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests} when the feature is ready to merge, or when developer feel that he/she need help or comments from other team means on this feature. Others also do the code review in the pull request. Pull request allow others in a software team to inspect and comment on the change will be made before the developer merges his change to another branch.
\item When the code is reviewed and is good to be merged, the developer should merge the code to the master.
\item After the code of this feature is in the master, the code will and should be immediately deployed. There should not be any rollback in the master branch. If there are any issues within the newly merged code, a new commit or a new branch should be made to fix the issue rather than rollback on the master.
\item Master branch is always deployable. This means when deploying the continuous delivery pipelines in our toolchain, only the master branch can be deployed to production. Moreover, there should not have any code which is not good to be deployed in the master branch.
\end{itemize}
\par
Note that in our Git workflow, there are several time points that we need to run the continuous delivery pipeline within the toolchain. The continuous delivery pipeline will also vary with the time point within the version control workflow. We will introduce this in detail on Section \ref{our-ci}.
\subsection{Continuous Delivery Pipeline}
\label{our-ci}
Figure \ref{fig:overview} shows the six Jenkins stages in our pipeline. The bottom part of this figure shows the task distribution between the master node and agent nodes. The master node is an EC2 virtual machine while agents run on Fargate instances within an ECS cluster. In section 2.3.1 We mentioned that the development of the monitoring system should be in parallel with the main software project, thus in our pipeline, the build and deploy of the monitoring system is in parallel with the case project.
\begin{figure}[!h]
     \centering
     \includegraphics[width=0.95\textwidth]{pics/overview.png}
     \caption{The Stages and Distributed Build in Our Pipeline}
     \label{fig:overview}
    \end{figure}
\par
As we can see from the Figure, when the master node starts a job, it will create a Docker container in AWS Fargate as the agent. The agent will pull codes from VCS, build the code, and then send the build artifacts back to the master node. After this, the container will be terminated. The master node will continue the rest steps.
\paragraph[]{Build Agents}
Build agent is an independent computation unit (VM or Docker container) that could exchange data with the Jenkins master node and run a certain part of the pipeline. To implement a Jenkins build cluster, we need first to implement build agents.
We discussed why we use Docker-based agent in our Jenkins build a cluster on \ref{docker} and we decide to it in our implementation. The first step of our implementation is to develop our own Docker image \footnote{The Docker image we developed could be found at https://hub.docker.com/r/dry1995/jnlp} of the Jenkins agent. We use the "jenkins/inbound-agent"\footnote{https://hub.docker.com/r/jenkins/inbound-agent/} as the base image, this allows our Jenkins agent to establish an inbound connection to the Jenkins master with TCP. The next step is to set up the built environment within the agent. We add shell script for auto-install all build dependencies of our case project when we build this Docker image. In the last step, we build the Docker image for build agent and push it to our registry\footnote{https://hub.docker.com/u/dry1995} in DockerHub. 
\par
We also discussed how Fargate allows us to run container serverless. To make use of Serverless offering of AWS, we let Docker-based Jenkins agents run on AWS Fargate to cut the operational effort and automate the scaling of Jenkins cluster. To implement this, we use Jenkins plugin "Amazon Elastic Container Service (ECS) / Fargate"\footnote{https://plugins.jenkins.io/amazon-ecs/}, which is the Jenkins plugin allow us to host Jenkins agent in Fargate.  
\paragraph[]{Considerations in Designing the Workflow of Distributed Pipeline}
The considerations behind to our design are that the first two steps take most of the time in our pipeline. In addition, according to Figure \ref{fig:pipeline}, these 2 steps runs more frequently than other steps, which the reason will be discussed in next section "Workflow in Production". The running time will be further extended when building a larger project. These two stages will be the bottleneck of the pipeline if we have it on the master mode. So we need to offload these steps to Jenkins agents for better performance.
\par
The second reason is: As we mentioned in the introduction to Docker on \ref {docker}, the built environment inside the Jenkins agent running in a Docker container is easier to change.
When the team wants to build the release for different OS (Which happens in C/C++ development) or wants to have a different build environment for various projects, Docker could help to eliminate tasks such as configuration and installation different environment. With Docker, they can just modify the Dockerfile that defines the Docker image of the Jenkins agents. 
However, we cannot put the stage of builds the Docker image in Jenkins agents. This is because AWS Fargate does not allow Docker container runs in privileged mode, which means we cannot use Docker within the container that runs in Fargate. This is one significant limitation of Fargate. Thus, we have to move the step back to the master node. Fortunately, in our case project, Docker build only takes a short time (average <1s). Therefore, this will not slow down the entire pipeline.
\par
We also notice that the Deploy stage also takes a long time. Still, we do not have it in the distributed build because: first, it is on the end of a pipeline so it will not block the further steps, second, the pipeline runs the stage less frequently than first two stages as shown in Figure \ref{fig:pipeline}. Thus there will be less possibility that there are many jobs runs at "Deploy" stage in parallel.
\begin{figure}[h]
 \centering
 \includegraphics[width=0.99\textwidth]{pics/pipeline.png}
 \caption{The Workflow of Continuous Delivery Pipeline in Our DevOps Toolchain}
 \label{fig:pipeline}
\end{figure}
\paragraph[]{Workflow for Continuous Delivery}
\label{workflow}
Figure \ref{fig:pipeline} shows our proposed workflow of a project that goes through the continuous delivery pipeline.
We can see when the event on the feature branch triggers the pipeline, and it only runs through the first two stages. This is because according to the practices of continuous integration mentioned by us in \ref{CD} and by Martin Fowler in \cite{fowler2006continuous}, a developer should merge (the "integration" in continuous integration) his/her work into main branch couple times per day. Therefore the code with this new feature runs through the whole pipeline at least several times a day. This already ensures the code could frequently be tested and deployed into the test environment.
\par
The developer only commits to the feature branch. The pipeline runs first two stages after a developer pushes local commits to Git. It first pulls the newly pushed code, and then build. In the build stage, the code first is analysed, then we do unit test to make sure the code could pass the test cases defined by the developer during development. In the end, the code will be built into Java ARchive file (.jar). The purpose of putting the code analysis step before the build is that the code analysis will check for warnings, errors, and code quality so that we can ensure that the code is runnable without syntax errors and pass the quality gate before putting it in the build. This build will not run the code exists error or not passing the quality gate set by the team. This measure can reduce the cost by lowering pipeline runtime if code is not runnable or not quality acceptable. 
\par
If no error returns after finishing all the above steps, the developer can open a pull request view the code change and ready to merge the code to the dev branch. Before the merge, the pull request needs to pass the code review by another developer. The code review is to make sure that the automated tests do not miss any bugs. After the code review passed, the reviewer or the developer him/herself merge the code to the dev branch.  
\par
After the code merged to the dev branch, the pipelines run again, this time it runs the whole pipeline. First, the pipeline executes the first two stages as in the feature branch. Now we have the Java ARchive file. The Java ARchive is an executable package of our Spring Boot application. Next step is to Dockerizing our application which generates the Docker image our application. Then we push the image to the Amazon Elastic Container Registry AWS (AWS ECR) for further use.
\par
\label{deploy}
The next step of the pipeline is deployment, the pipeline pull image in ECR that we pushed in the last stage, and then deploy it to the deployment environment in ECS with AWS CLI. The deployment strategy we are using is the rolling update. In the rolling update, we are gradually replacing instances in our deployment environment with the newer version of code.
\begin{figure}[!h]
     \centering
     \includegraphics[width=0.98\textwidth]{pics/test_result.png}
     \caption{API Test Result (Jenkins log output)}
     \label{fig:test-result}
    \end{figure}
\par
In the dev branch, the pipeline will deploy the application to the staging environment. The deployment to staging environment should be automated. This is because the staging environment is only for testing and only visible within the team. 
In the staging environment, we will conduct API testing (last stage shows in Figure \ref{fig:overview}) for test if our deployed API works and if it works as expected. The test is being done by trigger a Lambda function. The Lambda function sends a test HTTP request to the deployed endpoints, and verify if the HTTP response is correct. Figure \ref{fig:test-result} shows the test result of our case project. From the figure we can see if the test case is passed, and part of the HTTP response for manual inspection. If the deployed function passes the test, this shows the deployment works as expected and ready for the deployment. The developer could now open a pull request, merge code to master branch. The pipeline will deploy the application to the production environment, which is visible to the customers.
\subsection{Deployment Environment}
We create a simple deployment environment with AWS Elastic Container Service and Elastic Load Balancer. Same with Jenkins agents, we use Fargate to host our containerized case project. 
\par
AWS Fargate allow us to run our containerized application without having to manage servers, makes it easier for us to build a functionality complete DevOps toolchain implementation. We chose ECS instead of EKS (Elastic Kubernetes Service) because ECS is free of charge for have the ECS cluster running, while EKS charges an extra fee for the running time of the EKS cluster. Compared with EKS, ECS also provides better integration with other AWS services, such as AWS DevOps toolchain and out-of-box support for AWS CloudWatch monitoring.
\begin{figure}[h]
     \centering
     \includegraphics[width=0.99\textwidth]{pics/deploy.png}
     \caption{Deployment Environment}
     \label{fig:deploy}
\end{figure}
\par
Figure \ref{fig:deploy} shows our deployment architecture. The deployment region in Stockholm (EU-north-1). The Fargate instances in ECS cluster are automated scaled according to the number of incoming requests.
To improve the availability of the product, we deploy the case project into two different availability zones within the region.
When one availability zone is down, the load balancer can route the request to ensure that the request can still reach the healthy availability zone. Besides the availability improvement, the load balancer also distributes incoming requests across Fargate instances which maximizes the resources utilization rate within our ECS cluster.
% During deployment, we use Martin Fowler's blue and green deployment strategy \cite{fowler2010bluegreendeployment}, which is natively supported by ECS. This means when a new deployment comes, the older version will continue serving until the newer version reaches the stable status. This could significantly reduce the downtime in the deployment. 
% \subsection{Smoke Testing}
% Testing is an implement component within the DevOps pipeline
% In this section, we further discuss the smoke testing that we mentioned in the \ref{deploy}.
\subsection{Monitoring}
Different from test automation which usually integrated with the continuous delivery pipeline, the monitoring is independent of the pipeline. Usually, monitoring does not act as one step within the continuous delivery pipeline but as an independent component.
\par
In Chapter 3, we introduced AWS CloudWatch as one of the serverless services in AWS. In our toolchain, we will use it as a tool for monitoring the cloud infrastructure. With Cloudwatch, we can get the realtime log and quantitative metrics from our deployed container in the ECS. Figure \ref{fig:monitoring} shows the monitoring dashboards in CloudWatch and Grafana.
\begin{figure}[!tbp]
     \centering
     \begin{minipage}[b]{0.47\textwidth}
       \includegraphics[width=\textwidth]{pics/monitoring.png}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.50\textwidth}
       \includegraphics[width=\textwidth]{pics/grafana.png}
     \end{minipage}
     \caption{The Monitoring Dashboard of CloudWatch (Left) and Grafana (Right)}
     \label{fig:monitoring}
   \end{figure}
\par
In the last paragraph of Section 4.2.2, we mentioned that the CloudWatch is not capable of monitoring what happens inside the application framework, in our case, Spring boot. Thus, we use Prometheus to gather the metrics within our Spring Boot application. Prometheus gather the realtime metrics with the help of Micrometer plugin of Spring boot. Micrometer exports all numeric metrics in realtime; these metrics include JVM statistics, CPU/RAM utilization, number of request to each API endpoint and the user's self-defined metrics within Java code. Prometheus records these metrics every second, and save the metrics in time series to Database. In addition to Prometheus, we use Grafana to read the metrics from Prometheus and display the metrics on the dashboard.
\par
Another service we introduced in Chapter 3 is AWS lambda and discussed how could it be used in our DevOps toolchain in which monitoring is one of the use cases. In our monitoring system, AWS lambda is used as an extension for CloudWatch, and we use it for two cases.
\paragraph[]{Auto-Scaling the ECS Cluster with Custom Alarm in Cloudwatch}
As we mentioned in \ref{fig:deploy} Deployment Environment, The deployment could be auto-scaled by defining the auto-scaling policy within the ECS cluster. Such auto-scaling in practice is: When the watched resources utilization is above/below a certain threshold, an alarm in Cloudwatch will be triggered. The alarm will further trigger the scaling event if the scaling policy was being set before.
\par
% Nevertheless, in real-life development, many projects are in microservices architecture, rather than homogeneous architecture as we have in the case project. Microservices architecture means the project composed of multiple different services which running in different VM or Docker instance. Thus different services could have very different metrics such as CPU utilization. 
% To project with such architecture, the scaling policy is not flexible enough; it only based on thresholds on certain metrics such as CPU utilization and memory utilization. 
According to Luca Tiozzo's article \cite{AWSECSho47:online}, with Cloudwatch alarm based scaling, the developer team has to use two groups of alarm watching RAM and CPU, respectively. When the ECS cluster lack of CPU resource but not lack of RAM resource, the CPU alarm is triggered, and the ECS scaled up. Now the ECS cluster has enoxugh CPU resource, but the problem is, it may have too much RAM resource so that it triggers the RAM alarm to scale-in. So the cluster will scale in again. This will cause the cluster to keep scaling up and back without finding and suitable size.
\par
A good practice solves the problem is to use a single group of alarm that only triggered by single metrics \cite{AWSECSho47:online}. The developer team can set an AWS lambda function that read different metrics and then aggregate it to a single custom metric. The software team determines the threshold and metrics according to the deployed project. Once the aggregated metric reach the threshold, the lambda function triggers an alarm that can trigger the scaling of ECS cluster. 
\paragraph[]{Custom Project-Specific Metrics}
The second application scenario is related to the first one. The Cloudwatch has support on recourse utilization metrics. However, some metrics are project-specific and not related to resources utilization and performance. For example, the number of successful payment has been made in a payment service. In such a case, Lambda could fill the gap within the scope of CloudWatch. The team could set up a Lambda function which gets the number by monitoring the log with PutMetricData provided by CloudWatch. This Lambda can further forward the metrics to metrics analysis and visualization platform, for example, Grafana \footnote{https://grafana.com/grafana/} to give the management team an overview of the KPIs.
\section{Design of Integrated Serverless DevOps Toolchain}
AWS provides a set of serverless DevOps tools which could help us build a completely serverless DevOps toolchain. We introduced these tools in Chapter 3. In the section, we introduce the design of Serverless toolchain based on DevOps tools of AWS. Part of the toolchain is the same as the non-integrated DevOps toolchain in the previous section: some such as CloudWatch are already AWS serverless tools, some such as GitHub cannot be replaced with AWS tools, and some are tools embedded in the build pipeline, such as Gradle. Therefore we will not introduce these components but will focus on how do we make use AWS DevOps toolchain. Figure \ref{fig:codepipeline} shows the general workflow of a project delivered by our integrated DevOps toolchain.
\begin{figure}[h]
 \centering
 \includegraphics[width=0.90\textwidth]{pics/codepipeline.png}
 \caption{Integrated Serverless DevOps Toolchain}
 \label{fig:codepipeline}
\end{figure}
\subsection{Continuous Delivery Pipeline with AWS CodePipeline}
\label{codedeploy}
The workflow of our continuous delivery pipeline is the same as the pipeline in \ref{CD}. Instead of Jenkins, which is server-based, we build the pipeline with AWS CodePipeline. Figure \ref{fig:codepipeline} shows the activity within the CodePipeline in a single graph. Different from Jenkins who can do the whole continuous delivery process solely with the help of plugins, the CodePipeline just provides a platform which the development team can configure a workflow with AWS DevOps tools or other third-party tools. 
\begin{figure}[h]
     \centering
     \includegraphics[width=0.75\textwidth]{pics/cp-interface.png}
     \caption{Our Workflow in CodePipeline}
     \label{fig:cp-edit}
    \end{figure}
\par
% Same within the non-integrated version, we used GitHub as the version control system, and it has the same role as it has in the non-integrated toolchain. Although AWS also provides version control solution which is CodeCommit. It still lacks the functionality of collaboration compared with GitHub. Also, GitHub is already a serverless solution with good integration with AWS DevOps tools, so we do not need to change our version control system away from GitHub. 
We use CodeCommit to replace GitHub as the version control system, CodeCommit is a fully-managed source control services \cite{AWSCodeC33:online} from AWS, which hosts Git repositories. Same with GitHub, it helps used managed the server which hosting Git repositories and eliminates the need to managed and scale the infrastructures. It also supports the pull request and code review, which is necessary for work under Github flow. 
\par
In the next step, we are using AWS CodeBuild, which we introduced in Chapter 3. AWS CodeBuild does the same procedure as in Jenkins pipeline. It does code analysis, unit test and builds the Java application with Gradle, build the Docker image of the application and push to the ECR. The CodeDeploy deploys our application to ECS with Blue-and-green deployment strategy.
\par
The implementation of continuous delivery in CodePipeline is straightforward compared with Jenkins. In Jenkins, without the help of the plugin, the workflow of a continuous delivery pipeline can only be defined by groovy code, while CodePipeline natively provides a graphical user interface for continuous delivery pipeline modelling. 
\subsection{Source Control with AWS CodeCommit}
We use CodeCommit to replace the GitHub that we used in the non-integrated toolchain based on the following reasons.
\par
First, within CodePipeline, it is faster to clone a project from AWS CodeCommit than from GitHub. Our test shows that it takes CodePipeline on average 6 seconds to clone the case project from GitHub. However, the average clone time from CodeCommit is only 3 seconds.
\par
Second, CodeCommit has better integration with AWS. CodeCommit supports manage each user's access to the Git repositories with AWS Identity and Access Management (IAM). IAM is a centralized tool to manage each user's permission to all different AWS services. It is clear that IAM cannot manage the user's access to GitHub but can manage user's access to CodeCommit repositories. We think it is always good that the team could manage the access to all component within the DevOps toolchain. Thus use CodeCommit instead, Github can ensure centralized access management within the DevOps toolchain.
\par
Third, it is easier to use CodeCommit within the CodePipeline. CodeCommit could be added into AWS CodePipeline by select the repositories that the project located, while GitHub requires an additional manual login process.
\subsection{Build and Test with AWS CodeBuild}
Same with the design of our Jenkins pipeline, AWS CodeBuild also executes the build within Docker container. The image of the Docker container provided by AWS already contains environment for the build of different programming languages. It also includes the Java environment and Gradle which needed by our case project. Therefore we could save time in setting up the pipeline since we do not need to define the Docker image for the build by ourself.
\par
As we mentioned in Section \ref{codedeploy}, the process within CodeBuild is the same as we have in Jenkins before the stage "Deploy". We will not describe the process again here. Same with Jenkins, the workflow of CodeBuild is defined in a YAML configuration file. The only difference in the build workflow is that CodeBuild stores the build artifacts to S3. The build artifacts stored in S3 are configuration files, which define the deployment configuration in CodeDeploy. This is because CodeDeploy requires the deployment configuration from the step before it to run automatically.
\subsection{Blue/Green Deployment with AWS CodeDeploy}
One of the advantages of AWS DevOps tools is good integration with other AWS services. During the design and implementation of our toolchain, this advantage shows in the deployment to ECS with CodeDeploy.
\par
In Jenkins, there is a lack of specific plugin that helps us deploy the project into ECS or EKS. Thus we have to deploy our project to ECS with AWS command-line interface(CLI). The problem with AWS CLI is that it only supports the most basic deployment strategy, which is rolling update deployment. The rolling deployment strategy is to replace the old code running on the instances with new code gradually, instance by instance.
Such a difference shows better integration between CodeDeploy and AWS infrastructure, which allows us using more advanced deployment strategies. 
\begin{figure}[h]
 \centering
 \includegraphics[width=0.99\textwidth]{pics/bg.png}
 \caption{Blue and Green Deployment for Our Deployment Solution}
 \label{fig:bg}
\end{figure}
\par
In real-life production, the team would like to make sure the deployment is reliable with minimized downtime. Thus the safety is highly valued in deployment strategy. In answering this need, a strategy called blue/green deployment, which is now widely used in the industry. AWS CodeDeploy natively supports the blue/green strategy.
A blue/green deployment is a deployment strategy that requires two sets of totally identical deployment environment that runs the new and original version of code respectively, while the load balancer is gradually routing more incoming requests to the environment that runs the newer version of code. Blue/green deployment could minimize the downtime to nearly zero\cite{10.1007/978-3-030-45989-5_6}.
\par
Figure \ref{fig:bg} shows the visualisation of blue/green deployment. It also shows our design on how to implement blue/green deployment with CodeDeploy (shown before in Figure \ref{fig:deploy}). CodeDeploy controls routing policy within the load balancer. When new deployment comes, CodeDeploy does the following steps:
\begin{itemize}
 \item Provisioning new identical deployment environment (replacement environment) and deploy a newer version of code on it. In ECS, the deployment is called "task set".
 \item Control the load balancer, rerouting incoming traffic gradually to replacement ECS task set. The rerouteing rule is configurable according to the need. For example, the default rule is 10\% per minutes. This means there will be 10\% more requests from the client will be routed to the replacement ECS task set every minute until all the traffic has been routed to the replacement task set. We do not rerouteing all traffic at once to ensure the service will not fully down if the new deployed task set not works properly. This minimizes the downtime of our deployment.
 \item Wait for a certain time (depends on the deployed project) after rerouting is done. During the rerouteing, the load balancer keeps doing the health check to the new deployment by sending a request to the health check API endpoint. CodeDeploy read the health status from the load balancer. If the replacement tasks set is un-healthy, CodeDeploy does rollback by rerouting incoming traffic back to origin tasks set.
 \item If the new deployment is still healthy after waiting time, CodeDeploy terminates the running origin tasks set, and this means the old deployment environment is removed. Now the whole deploy process is done.
\end{itemize}
 When the error happens with a newer version of code, with blue/green deployment, we can immediately roll back to the last version by switching the rerouteing to Blue \cite{UsingBlu65:online}. While in the rolling update we are using in Jenkins pipeline, we have to redeploy the previous version. This difference reflects the better failed-safe of blue/green deployment. Under the same circumstance, the rollback with the rolling update is nevertheless taking a too long time, since we have to replace the already deployed code to the previous version. This could cause longer downtime of the server. However, because an identical environment must be run, blue/green deployments also bring more costs.
\begin{figure}[!tbp]
     \centering
     \begin{minipage}[b]{0.50\textwidth}
       \includegraphics[width=\textwidth]{pics/codedeploy_steps.png}
     \end{minipage}
     \hfill
     \begin{minipage}[b]{0.48\textwidth}
       \includegraphics[width=\textwidth]{pics/codedeploy_time.png}
     \end{minipage}
     \caption{Part of the CodeDeploy Dashboard, Deployment Status (Left) and Deployment Timeline (Right)}
     \label{fig:codedeploy_steps}
\end{figure}
\par
The better integration of CodeDeploy with the rest of AWS also brings benefit in the monitoring of the deployed solution. Aside from the existing monitoring with CloudWatch, CodeDeploy also provides us with a dashboard to show the deploy progress and the traffic rerouting process. Figure \ref{fig:codedeploy_steps} shows the dashboard of CodeDeploy that shows the status of our case project during the deployment.
\par
In comparison, with our non-integrated toolchain, we can not easily do the blue/green deployment. A possible solution is to set up an AWS Lambda function which will be triggered after the deployment. The invoked Lambda function control the load balancer to gradually redirect the traffic from the previous deployment to the new deployment. At the same time, the second lambda function continuously read the health status of the new deployment from the load balancer, and trigger the rollback if the new deployment is unhealthy. 
Besides all these, a dashboard is needed for the developer to monitoring the rerouteing status. On the contrary, in CodeDeploy, all the above tasks are being taken cared for. There is fully automated routing control on the load balancer, a dashboard that can monitor the realtime traffic percentage to blue and green instance, as well as the warning once the new deployment is failed.
% \section{Cloud Services}
% \label{assumption}
% In this section, we will introduce several could service from CH3 that could be helpful to the DevOps toolchain. 
% //  Using services in AWS as an example, Introduces how cloud services could improve. describe services in one section
% \subsection{Managed Container Services for Distributed Builds} 

% // Describe how AWS Fargate could Help
% \subsection{Serverless computing}
% // Describe how AWS lambda could Help and why do we choose it
% \subsection{...}
\subsection{Integration of AWS DevOps Tools using CodeStar}
In Section \ref{codedeploy}, we introduce how do we integrate different stages in the continuous delivery pipeline (CD pipeline) with CodePipeline. However, an integrated continuous delivery pipeline is not called integrated toolchain. The DevOps toolchain is centred with the continuous delivery pipelines, but the pipeline is not all of the toolchains. To integrate other AWS tools with the CD pipeline and get the integrated toolchain as a single application, we use CodeStar.  
\par
Figure \ref{fig:codestar} show the user interface of the CodeStar dashboard. We can see, besides the CodePipeline, the CodeStar also integrate tools like monitoring, project management, and version control and then compose our integrated toolchain. In conclusion, the AWS integrated toolchain includes the following tools:\label{codestar}
\begin{itemize}
     \item \textbf{CodeStar}: Integrate below tools into a single toolchain project management functionality. The project management could be extended by integrating with third-party tools, i.e. Jira.
          \item \textbf{CodePipeline}: Modelling Continuous Delivery Workflow, integrate tools that used within the CD pipeline.
          \begin{itemize}
               \item \textbf{CodeCommit}: Version control. Git repository which store the source code. Supports Team collaboration through branching, pull request and code review.
               \item \textbf{CodeBuild}: Build code of the software project.
               \item \textbf{CodeDeploy}: Deployment and monitoring. 
          \end{itemize}
          \item \textbf{CloudWatch}: Monitoring deployed solution.
\end{itemize}
\begin{figure}[h]
     \centering
     \includegraphics[width=0.99\textwidth]{pics/codestar.png}
     \caption{CodeStar Dashboard}
     \label{fig:codestar}
    \end{figure}

\section{Comparison between Integrated and Non-integrated Toolchain}
In this section, we discuss the difference between these two kinds of toolchains. The scope of comparison will be limited within the scope of functionality and ease of implementation. Furthermore, it is only based on our experiences with the tools used in our implementation.
We summarize the difference between these two toolchains as in Table \ref{tab:toolchain}.
We will do more comparison related to the performance and cost in Chapter 5.
\begin{table}[h]
 \centering
 \begin{tabular}{|c|c|c|}
 \hline
 &
 \begin{tabular}[c]{@{}c@{}}Non-Integrated \\ Toolchain\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}Integrated \\ Toolchain\end{tabular} \\ \hline
    Open-source &
 \begin{tabular}[c]{@{}c@{}}Open-source solution\\ existed\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}No, usually hosted \\ commercial solution\end{tabular} \\ \hline
 \begin{tabular}[c]{@{}c@{}}Delivery \\ method\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}Each part is a stand-alone \\ tool either hosted or\\ on-promised,  depends \\ on the tools selection\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}As a single cloud \\ hosted software\end{tabular} \\ \hline
 \begin{tabular}[c]{@{}c@{}}Implementing \\ time\end{tabular} &
      Long &
      Short \\ \hline
 \begin{tabular}[c]{@{}c@{}}Operational \\ effort\end{tabular} &
      High &
      Low \\ \hline
 \begin{tabular}[c]{@{}c@{}}Visibility \\ on status\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}Depends on tools, a \\ well-integrated toolchain \\ could gives good overview\\ on the whole toolchain.\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}Easy to see the\\ status as a whole\\ without additional \\ implementation\\ effort, low visibility\\ on under-laying \\ server since it's \\ hosted solution\end{tabular} \\ \hline
 \begin{tabular}[c]{@{}c@{}}extensibility\\ and tool \\ selection\\ freedom\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}Free to select tools\\ for each part of the\\ toolchain.\end{tabular} &
 \begin{tabular}[c]{@{}c@{}}Limited integration\\ with third-party \\ tools\end{tabular} \\ \hline
 \end{tabular}
 \caption{Comparison of DevOps Toolchains}
 \label{tab:toolchain}
 \end{table}
\subsection{Implementation and Cloud Deployment}
The cloud-based integrated toolchain is delivered as a hosted solution in a serverless model. However, we noticed that the non-integrated toolchain could also be completely serverless if we are using hosted tools for all the components. 
\par
For example, in our solution, we only have a continuous integration pipeline which is on-promised and need to be deployed to the VM manually. If we replace Jenkins with some other hosted tools, for example, Travis CI\footnote{https://travis-ci.org/} we can actually build a fully hosted but non-integrated DevOps toolchain. But, for the following reasons, it is not a satisfactory solution; thus, a non-integrated toolchain usually has some on-promised modules that need operational effort and cloud knowledge.
\begin{itemize}
 \item The hosted tools, especially tools for continuous integration pipeline, are all closed-source commercial solution. This means there is not an open-source community like in Jenkins. The extensibility therefore limited, for example, AWS DevOps tools can usually integrate with the certain tools that partner with AWS. Besides, the commercial user always needs to pay for these hosted tools.
 \item Hosted tools run in the vendor's server, and it requests user log in to use this tool which brings extra integration difficulty. This means for two hosted toolchain to integrate, it not only need to do the integration in the data transfer but also needs to connect their account system, for example, with OAuth. This extra inconvenience makes most hosted DevOps tools only do the integration support with other most popular tools which largely limited the extensibility. AWS natively only support the tools belongs to the DevOps Partner solutions \footnote{https://aws.amazon.com/devops/partner-solutions/} integrate to the AWS DevOps tools. Is not easy for the user to develop a tool and use it directly without making an agreement with AWS.
\end{itemize}
Therefore, a non-integrated toolchain usually has some on-promised module in real-life use. In our deployment process, we find it requires a lot of work to put an on-promised tool to cloud, especially if the developer is not familiar with the cloud platform that deploys these tools. For example, when setting up the Jenkins cluster that is only one component of the DevOps toolchain, we need to do the following steps:
\begin{enumerate} 
 \item Create a cloud virtual machine (EC2 instances) for hosting the Jenkins master.
 \item Setup IAM role for Jenkins master VM, make sure it has access to other AWS recourse that needed during the build.
 \item Setup security group and networking for the VM makes sure it can be accessed from the internet but only accessible within the company's IP range, and only port needed is opened to the public.
 \item Install Jenkins in the VM. Research what plugin is needed and install necessary plugins.
 \item An tedious setup process for setup Jenkins cluster that supports the distributed build. This includes setup ECS cluster for a build agent. Although Terraform makes the provision of cloud resources easier, still, prior knowledge for AWS is needed. The experiences in AWS also need to correctly configure the ECS cluster that maximizing the performance of build agents.
 \item Develop a Docker image for the container that runs Jenkins agents.
 \item Setup integration with other tools in toolchains by finding correct plugins and configure these plugins. 
\end{enumerate}
Only after these steps, we can start using Jenkins within the DevOps toolchain.
In comparison, the core feature of the integrated toolchain in AWS is an out-of-the-box feature which means there is no previous cloud knowledge needed, and there is no deployment and environment configuration required before we use it. We are free from all the steps we mentioned above. 
\par
In conclusion, the integrated toolchain could save time
\subsection{Extensibility and flexibility}
The integrated toolchain is a hosted platform that runs by a vendor. Similar to we mentioned in Section 4.4.1, we find all the currently integrated toolchain are all commercial and closed-source and not friendly to third-party plugins. So the integration of their third-party tools is usually only limited to popular tools. For example, AWS DevOps tools only support 21 tools within it is "DevOps Partner Solutions" \footnote{https://aws.amazon.com/devops/partner-solutions/}.
\par
Nevertheless, different from the single hosted tools we mentioned in 4.1.1, a hosted integrated DevOps toolchain mostly has everything needed for DevOps lifecycle, so it does not need to be able to integrate with third-party tools. Still, the limitation in third party tool support might make the software team facing trouble when they want to use certain tools which are not very supported.
\par
A non-integrated toolchain allows the software team to pick any tools for each component, as long as those tools can be integrated. The tools in the toolchains could also be open-source, which allow the software team to modify the tools according to their need. For example, develop a plugin for Jenkins that allows the integration of internal company tools with Jenkins.
\par
In conclusion, in terms of extensibility and flexibility, non-integrated toolchains are better than integrated toolchain.
\subsection{Integration Between Tools}
As we mentioned in Section 4.4.1, sometimes it is hard for tools within a non-integrated toolchain to integrate, especially between the hosted tools. 
During our implementation, we also realized that, first, it requires some configuration work for tools to be able to work together. Secondly, sometimes the integrating could be buggy is the configuration was done properly. For example, in our toolchain, the ECS build agent cannot connect to the Jenkins master, because the networking within the ECS cluster was not correctly set. This means the software teams need further maintaining of the toolchain.
In integrated toolchain, the toolchains are delivered as a single cloud-based software, and each part naturally coped with each other, which makes integration much more straightforward.
The better integrating between each component also makes it easier to monitor the toolchain as a whole.
\subsection{Visibility}
\label{visibility}
In Section 4.4.3, we mentioned that the integrated toolchain is easier to be monitored as a whole, however, when comes to every single component, in our implementation, we find out that integrated toolchain is lack of visibility. We met difficulties with integrate/non-integrated toolchain respectively, and the experience in solving these two problems shows how visibility could affect the ease when it comes to finding where the problem is.
\par
The first problem was with the non-integrated toolchain, which we have full visibility to the underlying virtual machine. Was that Jenkins master was having difficulty in the provision and connect to the agents. Since Jenkins is a web service deployed in our EC2 virtual machine, we solve the problem easily by reading the Error message within the Jenkins log file.
\par
The second problem we met was within AWS CodeDeploy, which we have no visibility to the machine it runs on. The CodeDeploy failed to deploy the case project to the ECS cluster. We could not find the reason at that time since we cannot find the log of CodeDeploy anywhere since it is not shown in the web interface, and we have no access to the underlying cloud infrastructure to find the log file either. In the end, we have to reread the documentation and find our load balancer was not properly set so that CodeDeploy could do the health check. Thus the CodeDeploy cannot provision the new deployment environment for us.
\par
The lack of visibility is a problem with all hosted serverless services since the users do not have the visibility to the infrastructure behind the service.
\section{Challenges in Implementation and Design of DevOps toolchains}
In this section, we discuss the challenge that we met during the implementation. 
\subsection{Challenge I: The Enormous and Unregulated Jenkins Plugins System}
Jenkins has more than 1600 plugins which brings the amazing software extensibility, which is one of the main advantages of Jenkins. However, there are two problems with Jenkins' plugins; First, there are usually more than one plugins that have the same functionality, for example, there are at least five different plugins related to running Docker container as Jenkins agent. Second, most of the plugin is developed by the open-source community, compared with a commercial product, the open-source project is lack of support such as case support to the specific problem. Besides, the open-source project could end up without further support once the developer decides to discontinue the project.

During our implementation, we find out there are two plugins that support run Jenkins agent in ECS cluster. However, we find only one work after we tried both plugins. Besides, the documentation of Jenkins plugins sometimes is abysmal. For example, the documentation of the plugin that we use for Jenkins agent is too brief to tell us how to use the plugin, and it is not even mentioned the security setting needed in Jenkins master node that allows agents to connect the master node. 
\par
As a result of the above three factors, for a developer who does not has previous experience in build DevOps toolchain, we end up spending a very long time in selecting and configuring tools and trying to solve the problem which nether mentioned in the documentation and on the internet. 
\subsection{Challenge II: Fargate Does not Supports Container runs in Privileged Mode}
As mentioned in the title, this is a limitation in AWS Fargate to prevent containers from gaining access to critical resources on the host. As a result, we cannot use Docker within a Docker container that runs on Fargate. This makes it impossible for us to distribute the "Build docker image" and "Push to ECR" stages to agents. Instead, we have to run them on the master. Luckily, these two stages take a short time (less than 5s in total), so this limitation will not slow down the pipeline too much when multiple builds run in parallel.
\par
A possible solution solving this problem is to runs these two stages in AWS CodeBuild, AWS CodeBuild has support to Jenkins, which allow us to run certain Jenkins stages in CodeBuild. Moreover, CodeBuild supports fully parallel execution as in Fargate.
\subsection{Challenge III: Slow Starting Time for Agents in AWS Fargate}
On average it takes around 60s from sending a Jenkins job to agent, to running the job in an agent. To our case project that takes 90 seconds to go through the whole pipeline, this is a relatively long time. During this 60s, Jenkins master node sends task definition \footnote{Define specification of a container runs in ECS.} to ECS, provision a Fargate instance within ECS, then pull the image we developed for Jenkins agent, star the agent container within Fargate and then connect to the Jenkins master. This challenge is due to the nature of the serverless computing that we discussed in Chapter 2, and we believe there is not an economical way to solve the challenge with the current setup.
\subsection{Challenge IV: No Enough Visibility in AWS DevOps tools}
As we mentioned in Section \ref{visibility}, the lack of visibility of the underlying processes (especially in CodeDeploy) has brought some obstacles to our debugging pipeline. When we tried to deploy the case project to ECS via blue/green deployment, there was a problem: CodeDeploy got stuck in creating a replacement service. We know there was something wrong within either the configuration of ECS, load balancer, health-check or security/network setting. However, there is no log output of the underlying deployment process in CodeDeploy. Finally, we had to check all the possible causes of the problem one by one and found that it was a failed health check due to the configuration error in the load balancer. Such a way of detecting issues with the deployment was very time-consuming.